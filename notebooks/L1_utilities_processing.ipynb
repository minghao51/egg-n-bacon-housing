{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# procesing all utilities with additional latlon and discard extra info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "# import polars as pl\n",
    "import geopandas as gpd\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OneMap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create one map header"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import os\n",
    "import json\n",
    "\n",
    "url = \"https://www.onemap.gov.sg/api/auth/post/getToken\"\n",
    "\n",
    "payload = {\n",
    "    \"email\": os.environ['ONEMAP_EMAIL'],\n",
    "    \"password\": os.environ['ONEMAP_EMAIL_PASSWORD']\n",
    "}\n",
    "\n",
    "response = requests.request(\"POST\", url, json=payload)\n",
    "access_token = json.loads(response.text)['access_token']\n",
    "headers = {\"Authorization\": f\"{access_token}\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# School"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# config\n",
    "school_query_onemap = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "import pandas as pd\n",
    "from time import sleep\n",
    "from tqdm import tqdm\n",
    "from random import uniform\n",
    "\n",
    "\n",
    "def extract_df_data(school_df, search_cols, initial_backoff, max_retries, max_backoff, headers):\n",
    "    \"\"\"\n",
    "    Extracts data from OneMap API for each search_cols in a pandas DataFrame.\n",
    "\n",
    "    Args:\n",
    "        school_df (pandas.DataFrame): DataFrame containing a column named 'postal_code'.\n",
    "        initial_backoff (float): Initial delay for exponential backoff in seconds.\n",
    "        max_retries (int): Maximum number of retries for failed requests.\n",
    "        headers (dict): Dictionary containing headers for API requests.\n",
    "\n",
    "    Returns:\n",
    "        list: List of DataFrames containing extracted data for each successful request.\n",
    "    \"\"\"\n",
    "\n",
    "    df_list = []\n",
    "    for search_string in tqdm(school_df[search_cols], desc=\"Extracting Data\"):\n",
    "        retries = 0\n",
    "        success = False\n",
    "        backoff = initial_backoff\n",
    "\n",
    "        while not success and retries < max_retries:\n",
    "            try:\n",
    "                url = f\"https://www.onemap.gov.sg/api/common/elastic/search?searchVal={search_string}&returnGeom=Y&getAddrDetails=Y&pageNum=1\"\n",
    "                response = requests.get(url, headers=headers)\n",
    "                # print(f\"{search_string}\")\n",
    "                response.raise_for_status()  # Raise an exception for HTTP errors\n",
    "                data = json.loads(response.text)\n",
    "                _df = pd.DataFrame(data['results']).iloc[0:1]\n",
    "                _df['NameAddress'] = search_string\n",
    "                df_list.append(_df)\n",
    "                success = True\n",
    "\n",
    "            except:\n",
    "                # print(f\"No results found for postal code {search_string}. Retry.\")\n",
    "                retries += 1\n",
    "                backoff = min(backoff * 2, max_backoff)  # Exponential backoff\n",
    "                delay = backoff + uniform(0, 1)  # Add jitter to the delay\n",
    "                print(\n",
    "                    f\"Request failed for {search_string}. Retrying in {delay:.2f} seconds. (Retry {retries}/{max_retries})\")\n",
    "                sleep(delay)\n",
    "\n",
    "        if not success:\n",
    "            print(\n",
    "                f\"Failed to retrieve data for {search_string} after {max_retries} retries.\")\n",
    "\n",
    "    return df_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "import pandas as pd\n",
    "from time import sleep\n",
    "from random import uniform\n",
    "\n",
    "# Define maximum retries and initial/maximum backoff times\n",
    "max_retries = 3\n",
    "initial_backoff = 1  # seconds\n",
    "max_backoff = 32  # seconds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_school_data_from_onemap(school_df):\n",
    "    \"\"\"\n",
    "    Retrieves school data from OneMap API for each postal code in the 'postal_code' column of the provided DataFrame.\n",
    "\n",
    "    Args:\n",
    "        school_df (pd.DataFrame): DataFrame containing a 'postal_code' column.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame containing school data, including 'name', 'lat', 'lon', etc.\n",
    "    \"\"\"\n",
    "    df_list = extract_df_data(school_df,\n",
    "                              search_cols='postal_code',\n",
    "                              initial_backoff=initial_backoff,\n",
    "                              max_retries=max_retries,\n",
    "                              max_backoff=max_backoff,\n",
    "                              headers=headers)\n",
    "    if df_list:  # Check if any data was retrieved\n",
    "        df_school = pd.concat(df_list).rename({\n",
    "            'SEARCHVAL': 'name',\n",
    "            'LATITUDE': 'lat',\n",
    "            'LONGITUDE': 'lon',\n",
    "            'POSTAL': 'postal',\n",
    "            'ADDRESS': 'address'\n",
    "        }, axis=1)\n",
    "        df_school = df_school[['name', 'lat', 'lon', 'postal', 'address']]\n",
    "        df_school['type'] = 'school'\n",
    "        df_school['address'] = [i.lower() for i in df_school['address']]\n",
    "        df_school['name'] = [i.lower() for i in df_school['name']]\n",
    "    else:\n",
    "        print(\"No school data retrieved from OneMap API.\")\n",
    "\n",
    "    return df_school"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "School data read from parquet file.\n"
     ]
    }
   ],
   "source": [
    "if school_query_onemap:\n",
    "    school = pd.read_csv(\n",
    "        \"../data/raw_data/csv/datagov/Generalinformationofschools.csv\")\n",
    "    school_df = get_school_data_from_onemap(school)\n",
    "    school_df.to_parquet('../data/L1/school_queried.parqeut')\n",
    "\n",
    "else:\n",
    "    # Read from parquet file if not querying OneMap API\n",
    "    try:\n",
    "        school_df = pd.read_parquet(\"../data/L1/school_queried.parqeut\")\n",
    "        print(\"School data read from parquet file.\")\n",
    "    except FileNotFoundError:\n",
    "        print(\"School data parquet file not found. Please ensure it exists or query OneMap API.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# config\n",
    "mall_query_onemap = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "if mall_query_onemap:\n",
    "    mall = pd.read_parquet(\"../data/raw_data/datawiki_shopping_mall.parquet\")\n",
    "    df_list = extract_df_data(mall,\n",
    "                              search_cols='shopping_mall',\n",
    "                              initial_backoff=initial_backoff,\n",
    "                              max_retries=max_retries,\n",
    "                              max_backoff=max_backoff,\n",
    "                              headers=headers)\n",
    "    if df_list:  # Check if any data was retrieved\n",
    "        df_mall = pd.concat(df_list).rename({\n",
    "            'SEARCHVAL': 'name',\n",
    "            'LATITUDE': 'lat',\n",
    "            'LONGITUDE': 'lon',\n",
    "            'POSTAL': 'postal',\n",
    "            'ADDRESS': 'address'\n",
    "        }, axis=1)\n",
    "        df_mall = df_mall[['name', 'lat', 'lon', 'postal', 'address']]\n",
    "        df_mall['type'] = 'mall'\n",
    "        df_mall['address'] = [i.lower() for i in df_mall['address']]\n",
    "        df_mall['name'] = [i.lower() for i in df_mall['name']]\n",
    "\n",
    "        df_mall.to_parquet('../data/L1/mall_queried.parqeut')\n",
    "else:\n",
    "    df_mall = pd.read_parquet(\"../data/L1/mall_queried.parqeut\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kindergardens, gym, hawker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def extract_html_name(html_str: str, name_search) -> str:\n",
    "    \"\"\"\n",
    "    Extracts the name from an HTML string.\n",
    "\n",
    "    Args:\n",
    "        html_str (str): The HTML string to parse.\n",
    "\n",
    "    Returns:\n",
    "        str: The extracted name.\n",
    "    \"\"\"\n",
    "    soup = BeautifulSoup(html_str, 'html.parser')\n",
    "    name_cell = soup.find('th', string=name_search).find_next('td')\n",
    "    name = name_cell.text.strip()\n",
    "    return name\n",
    "\n",
    "\n",
    "def parse_datagov_geojson(path: str, data_type: str, name_search: str = 'NAME') -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Parses a GeoJSON file and extracts relevant data.\n",
    "\n",
    "    Args:\n",
    "        path (str): The file path to the GeoJSON file.\n",
    "        data_type (str): The type of data (e.g., \"kindergardens\").\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: A DataFrame with the extracted data.\n",
    "    \"\"\"\n",
    "    df = gpd.read_file(path)\n",
    "    df = df.to_crs('4326')\n",
    "    df[\"lat\"] = df['geometry'].y\n",
    "    df[\"lon\"] = df['geometry'].x\n",
    "    df['type'] = data_type\n",
    "    df['name'] = [extract_html_name(i, name_search) for i in df['Description']]\n",
    "    return pd.DataFrame(df[['name', 'type', 'lat', 'lon']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "kindergarden_df = parse_datagov_geojson(\n",
    "    \"../data/raw_data/csv/datagov/Kindergartens.geojson\", \"kindergarden\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "gym_df = parse_datagov_geojson(\n",
    "    \"../data/raw_data/csv/datagov/GymsSGGEOJSON.geojson\", \"gym\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "hawker_df = parse_datagov_geojson(\n",
    "    \"../data/raw_data/csv/datagov/HawkerCentresGEOJSON.geojson\", \"hawker\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "water_activities_df = parse_datagov_geojson(\n",
    "    \"../data/raw_data/csv/datagov/WaterActivitiesSG.geojson\", \"water_activities\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "supermarket_df = parse_datagov_geojson(\n",
    "    \"../data/raw_data/csv/datagov/SupermarketsGEOJSON.geojson\", \"supermarket\", \"LIC_NAME\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "preschool_df = parse_datagov_geojson(\n",
    "    \"../data/raw_data/csv/datagov/PreSchoolsLocation.geojson\", \"preschool\", \"CENTRE_NAME\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# park"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = gpd.read_file(\n",
    "    \"../data/raw_data/csv/datagov/NParksParksandNatureReserves.geojson\")\n",
    "df = df.set_crs(crs='epsg:4326')\n",
    "df = df.to_crs(crs=3857)\n",
    "df['lon'] = df.centroid.x\n",
    "df['lat'] = df.centroid.y\n",
    "df['type'] = 'park'\n",
    "df['name'] = [extract_html_name(i, 'NAME') for i in df['Description']]\n",
    "park_df = df[['name', 'type', 'lon', 'lat', 'geometry']]\n",
    "park_df.to_file(\"../data/L1/park.geojson\", driver='GeoJSON')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = gpd.read_file(\n",
    "    \"../data/raw_data/csv/datagov/MasterPlan2019SDCPParkConnectorLinelayerGEOJSON.geojson\")\n",
    "park_connector_df = df.drop('Description', axis=1)\n",
    "park_connector_df.to_file(\n",
    "    \"../data/L1/park_connector.geojson\", driver='GeoJSON')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "waterbody_df = gpd.read_file(\n",
    "    \"../data/raw_data/csv/datagov/MasterPlan2019SDCPWaterbodylayerKML.kml\")\n",
    "waterbody_df = waterbody_df.to_crs('3857')\n",
    "waterbody_df['area_m'] = waterbody_df.geometry.area\n",
    "waterbody_df = waterbody_df[waterbody_df['area_m'] >= 4000].reset_index()\n",
    "waterbody_df = waterbody_df[['Name', 'geometry', 'area_m']]\n",
    "waterbody_df.to_parquet('../data/L1/amenity.parqeut')\n",
    "waterbody_df.to_file(\"../data/L1/waterbody.geojson\", driver='GeoJSON')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Concat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_combined = pd.concat([\n",
    "    school_df[['name', 'type', 'lat', 'lon']],\n",
    "    df_mall[['name', 'type', 'lat', 'lon']],\n",
    "    kindergarden_df,\n",
    "    gym_df,\n",
    "    hawker_df,\n",
    "    kindergarden_df,\n",
    "    water_activities_df,\n",
    "    supermarket_df,\n",
    "    preschool_df\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_combined['lat'] = df_combined['lat'].astype('float')\n",
    "df_combined['lon'] = df_combined['lon'].astype('float')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "type\n",
       "preschool           2290\n",
       "kindergarden         896\n",
       "supermarket          526\n",
       "school               336\n",
       "mall                 169\n",
       "gym                  159\n",
       "hawker               125\n",
       "water_activities      32\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_combined['type'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_combined.to_parquet('../data/L1/amenity.parqeut')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "demo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
