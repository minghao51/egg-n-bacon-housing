{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# procesing all housing with additional latlon and discard extra info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pathlib\n",
    "import sys\n",
    "\n",
    "import pandas as pd\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Add src directory to path for imports\n",
    "sys.path.append(str(pathlib.Path(__file__).parent.parent / 'src'))\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "from data_helpers import save_parquet\n",
    "from geocoding import extract_unique_addresses, fetch_data, load_ura_files, setup_onemap_headers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load all URA and HDB transaction files using shared function\n",
    "csv_base_path = pathlib.Path(__file__).parent.parent / 'data' / 'raw_data' / 'csv'\n",
    "ec_df, condo_df, residential_df, hdb_df = load_ura_files(csv_base_path)\n",
    "\n",
    "# Save individual transaction datasets\n",
    "save_parquet(ec_df, \"L1_housing_ec_transaction\", source=\"URA CSV data\")\n",
    "save_parquet(condo_df, \"L1_housing_condo_transaction\", source=\"URA CSV data\")\n",
    "save_parquet(residential_df, \"L1_housing_residential_transaction\", source=\"URA CSV data\")\n",
    "save_parquet(hdb_df, \"L1_housing_hdb_transaction\", source=\"data.gov.sg CSV data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Combining to idenfity all unique condo and flats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract unique addresses using shared function\n",
    "housing_df = extract_unique_addresses(ec_df, condo_df, residential_df, hdb_df)\n",
    "\n",
    "# Display first 10 addresses\n",
    "for search_string in housing_df['NameAddress'][:10]:\n",
    "    print(search_string)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OneMap Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup OneMap authentication using shared function\n",
    "headers = setup_onemap_headers()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Search for X, Y and other data on OneMap\n",
    "- this will take a while\n",
    "- with exponential backoff and limit to failure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "df_list = []\n",
    "failed_searches = []\n",
    "total_addresses = len(housing_df['NameAddress'])\n",
    "print(f\"Starting geocoding for {total_addresses} unique addresses...\")\n",
    "print(\"This may take 30+ minutes due to API rate limiting\")\n",
    "\n",
    "for i, search_string in enumerate(housing_df['NameAddress'], 1):\n",
    "  try:\n",
    "    # Use fetch_data from shared geocoding module\n",
    "    _df = fetch_data(search_string, headers)\n",
    "    _df['NameAddress'] = search_string\n",
    "    df_list.append(_df)\n",
    "\n",
    "    # Print progress every 50 addresses\n",
    "    if i % 50 == 0:\n",
    "      print(f\"Progress: {i}/{total_addresses} addresses ({i/total_addresses*100:.1f}%)\")\n",
    "\n",
    "  except requests.RequestException:\n",
    "    failed_searches.append(search_string)\n",
    "    print(f\"❌ Request failed for {search_string}. Skipping.\")\n",
    "\n",
    "print(f\"✅ Completed geocoding: {len(df_list)}/{total_addresses} successful\")\n",
    "\n",
    "if failed_searches:\n",
    "  print(f\"⚠️  Failed to retrieve data for {len(failed_searches)} addresses\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import time\n",
    "# import random\n",
    "\n",
    "# for search_string in housing_df['NameAddress']:\n",
    "#     retries = 0\n",
    "#     success = False\n",
    "#     backoff = initial_backoff\n",
    "\n",
    "#     while not success and retries < max_retries:\n",
    "#         try:\n",
    "#             url = f\"https://www.onemap.gov.sg/api/common/elastic/search?searchVal={search_string}&returnGeom=Y&getAddrDetails=Y&pageNum=1\"\n",
    "#             response = requests.request(\"GET\", url, headers=headers)\n",
    "#             response.raise_for_status()  # Raise an exception for HTTP errors\n",
    "\n",
    "#             _df = pd.DataFrame(json.loads(response.text)['results']).reset_index().rename(\n",
    "#                 {'index': 'search_result'}, axis=1)\n",
    "#             _df['NameAddress'] = search_string\n",
    "#             df_list.append(_df)\n",
    "\n",
    "#             success = True\n",
    "\n",
    "#         except requests.RequestException as e:\n",
    "#             retries += 1\n",
    "#             backoff = min(backoff * 2, max_backoff)  # Exponential backoff\n",
    "#             # Add some jitter to the delay\n",
    "#             delay = backoff + random.uniform(0, 1)\n",
    "#             print(\n",
    "#                 f\"Request failed for {search_string}. Retrying in {delay:.2f} seconds. (Retry {retries}/{max_retries})\")\n",
    "#             time.sleep(delay)\n",
    "\n",
    "#     if not success:\n",
    "#         print(\n",
    "#             f\"Failed to retrieve data for {search_string} after {max_retries} retries.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate the dataframes\n",
    "df_housing_searched = pd.concat(df_list)\n",
    "\n",
    "# Save the full dataset\n",
    "save_parquet(df_housing_searched, \"L2_housing_unique_full_searched\", source=\"L1 transaction data\")\n",
    "\n",
    "# Filter for search_result == 0 and reset index\n",
    "df_housing_searched_selected = df_housing_searched[df_housing_searched['search_result'] == 0].reset_index(drop=True)\n",
    "\n",
    "# Add the 'property_type' column\n",
    "df_housing_searched_selected['property_type'] = housing_df['property_type']\n",
    "\n",
    "# Save the filtered dataset\n",
    "save_parquet(df_housing_searched_selected, \"L2_housing_unique_searched\", source=\"L2 housing data\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "demo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
