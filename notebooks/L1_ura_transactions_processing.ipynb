{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# procesing all housing with additional latlon and discard extra info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "ec_list = [\n",
    "    \"URA_ResidentialTransaction_EC2020_20240917220317\",\n",
    "    \"URA_ResidentialTransaction_EC2021_20240917220358\",\n",
    "    \"URA_ResidentialTransaction_EC2022_20240917220420\",\n",
    "    \"URA_ResidentialTransaction_EC2023_20240917220459\",\n",
    "    \"URA_ResidentialTransaction_EC2024_20240917220523\",\n",
    "]\n",
    "\n",
    "condo_list = [\n",
    "    \"URA_ResidentialTransaction_Conda2020_20240917220234\",\n",
    "    \"URA_ResidentialTransaction_Conda2021_20240917220149\",\n",
    "    \"URA_ResidentialTransaction_Conda2022_20240917220116\",\n",
    "    \"URA_ResidentialTransaction_Conda2023_20240917215948\",\n",
    "    \"URA_ResidentialTransaction_Condo2024_20240917215852\",\n",
    "]\n",
    "\n",
    "hdb_list = [\n",
    "    \"ResaleFlatPricesBasedonRegistrationDateFromJan2015toDec2016\",\n",
    "    \"ResaleflatpricesbasedonregistrationdatefromJan2017onwards\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "ec_df = [pd.read_csv(f\"../data/raw_data/csv/ura/{ec}.csv\") for ec in ec_list]\n",
    "ec_df = pd.concat(ec_df)\n",
    "ec_df.to_parquet(r\"../data/L1/housing_ec_transaction.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "condo_df = [\n",
    "    pd.read_csv(f\"../data/raw_data/csv/ura/{condo}.csv\") for condo in condo_list\n",
    "]\n",
    "condo_df = pd.concat(condo_df)\n",
    "condo_df['Area (SQM)'] = condo_df['Area (SQM)'].str.replace(\n",
    "    ',', '').str.strip()\n",
    "condo_df['Area (SQM)'] = pd.to_numeric(condo_df['Area (SQM)'], errors='coerce')\n",
    "condo_df.to_parquet(r\"../data/L1/housing_condo_transaction.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "hdb_df = [pd.read_csv(f\"../data/raw_data/csv/datagov/{hdb}.csv\")\n",
    "          for hdb in hdb_list]\n",
    "hdb_df = pd.concat(hdb_df)\n",
    "\n",
    "\n",
    "def standardize_lease_duration(lease):\n",
    "    if isinstance(lease, int) or lease.isdigit():\n",
    "        return int(lease) * 12  # assume months\n",
    "    else:\n",
    "        match = re.match(r'(\\d+) years?\\s*', lease)  # (\\d+) months?\n",
    "        if match:\n",
    "            years = int(match.group(1))\n",
    "            # months = int(match.group(2)) if match.group(2) else 0\n",
    "            return years * 12  # + months\n",
    "        else:\n",
    "            return None  # or raise an exception\n",
    "\n",
    "\n",
    "hdb_df['remaining_lease_months'] = hdb_df['remaining_lease'].apply(\n",
    "    standardize_lease_duration)\n",
    "hdb_df.drop('remaining_lease', axis=1, inplace=True)\n",
    "hdb_df.to_parquet(r\"../data/L1/housing_hdb_transaction.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Combining to idenfity all unique condo and flats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "condo_df['property_type'] = 'private'\n",
    "ec_df['property_type'] = 'private'\n",
    "hdb_df['property_type'] = 'hdb'\n",
    "\n",
    "\n",
    "housing_df = pd.concat(\n",
    "    [\n",
    "        condo_df[[\"Project Name\", \"Street Name\",\n",
    "                  \"property_type\"]].drop_duplicates(),\n",
    "        ec_df[[\"Project Name\", \"Street Name\", 'property_type']].drop_duplicates(),\n",
    "        hdb_df[[\"block\", \"street_name\", 'property_type']].drop_duplicates(),\n",
    "    ],\n",
    "    ignore_index=True,\n",
    ")\n",
    "\n",
    "NameAddress_list = [\"Project Name\", \"Street Name\",\n",
    "                    \"block\", \"street_name\"]\n",
    "for i in NameAddress_list:\n",
    "    housing_df[i] = housing_df[i].fillna(\"\")\n",
    "housing_df[\"NameAddress\"] = housing_df[NameAddress_list].agg(\" \".join, axis=1)\n",
    "housing_df[\"NameAddress\"] = [i.strip() for i in housing_df[\"NameAddress\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AFFINITY AT SERANGOON SERANGOON NORTH AVENUE 1\n",
      "THE FLORENCE RESIDENCES HOUGANG AVENUE 2\n",
      "THE GARDEN RESIDENCES SERANGOON NORTH VIEW\n",
      "THE PROMENADE@PELIKAT JALAN PELIKAT\n",
      "PRIMO RESIDENCES JALAN PELIKAT\n",
      "SENGKANG GRAND RESIDENCES COMPASSVALE BOW\n",
      "COMPASS HEIGHTS SENGKANG SQUARE\n",
      "A TREASURE TROVE PUNGGOL WALK\n",
      "BOTANIQUE AT BARTLEY UPPER PAYA LEBAR ROAD\n",
      "REGENTVILLE HOUGANG STREET 92\n"
     ]
    }
   ],
   "source": [
    "for search_string in housing_df['NameAddress'][:10]:\n",
    "    print(search_string)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OneMap Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import os\n",
    "import json\n",
    "\n",
    "url = \"https://www.onemap.gov.sg/api/auth/post/getToken\"\n",
    "\n",
    "payload = {\n",
    "    \"email\": os.environ['ONEMAP_EMAIL'],\n",
    "    \"password\": os.environ['ONEMAP_EMAIL_PASSWORD']\n",
    "}\n",
    "\n",
    "response = requests.request(\"POST\", url, json=payload)\n",
    "access_token = json.loads(response.text)['access_token']\n",
    "headers = {\"Authorization\": f\"{access_token}\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Search for X, Y and other data on OneMap\n",
    "- this will take a while\n",
    "- with exponential backoff and limit to failure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "import pandas as pd\n",
    "from tenacity import retry, wait_exponential\n",
    "\n",
    "df_list = []\n",
    "max_retries = 3\n",
    "initial_backoff = 1  # seconds\n",
    "max_backoff = 32  # seconds\n",
    "\n",
    "\n",
    "@retry(wait=wait_exponential(multiplier=1, min=initial_backoff, max=max_backoff))\n",
    "def fetch_data(search_string):\n",
    "  url = f\"https://www.onemap.gov.sg/api/common/elastic/search?searchVal={search_string}&returnGeom=Y&getAddrDetails=Y&pageNum=1\"\n",
    "  response = requests.get(url, headers=headers)\n",
    "  response.raise_for_status()\n",
    "  return pd.DataFrame(json.loads(response.text)['results']).reset_index().rename(\n",
    "      {'index': 'search_result'}, axis=1)\n",
    "  \n",
    "\n",
    "failed_searches = []\n",
    "for search_string in housing_df['NameAddress']:\n",
    "  try:\n",
    "    _df = fetch_data(search_string)\n",
    "    _df['NameAddress'] = search_string\n",
    "    df_list.append(_df)\n",
    "  except requests.RequestException as e:\n",
    "    failed_searches.append(search_string)\n",
    "    print(f\"Request failed for {search_string}. Skipping.\")\n",
    "\n",
    "if failed_searches:\n",
    "  print(f\"Failed to retrieve data for the following addresses after {max_retries} retries: {', '.join(failed_searches)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import time\n",
    "# import random\n",
    "\n",
    "# for search_string in housing_df['NameAddress']:\n",
    "#     retries = 0\n",
    "#     success = False\n",
    "#     backoff = initial_backoff\n",
    "\n",
    "#     while not success and retries < max_retries:\n",
    "#         try:\n",
    "#             url = f\"https://www.onemap.gov.sg/api/common/elastic/search?searchVal={search_string}&returnGeom=Y&getAddrDetails=Y&pageNum=1\"\n",
    "#             response = requests.request(\"GET\", url, headers=headers)\n",
    "#             response.raise_for_status()  # Raise an exception for HTTP errors\n",
    "\n",
    "#             _df = pd.DataFrame(json.loads(response.text)['results']).reset_index().rename(\n",
    "#                 {'index': 'search_result'}, axis=1)\n",
    "#             _df['NameAddress'] = search_string\n",
    "#             df_list.append(_df)\n",
    "\n",
    "#             success = True\n",
    "\n",
    "#         except requests.RequestException as e:\n",
    "#             retries += 1\n",
    "#             backoff = min(backoff * 2, max_backoff)  # Exponential backoff\n",
    "#             # Add some jitter to the delay\n",
    "#             delay = backoff + random.uniform(0, 1)\n",
    "#             print(\n",
    "#                 f\"Request failed for {search_string}. Retrying in {delay:.2f} seconds. (Retry {retries}/{max_retries})\")\n",
    "#             time.sleep(delay)\n",
    "\n",
    "#     if not success:\n",
    "#         print(\n",
    "#             f\"Failed to retrieve data for {search_string} after {max_retries} retries.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate the dataframes\n",
    "df_housing_searched = pd.concat(df_list)\n",
    "\n",
    "# Save the full dataset\n",
    "df_housing_searched.to_parquet(\"../data/L2/housing_unique_full_searched.parquet\")\n",
    "\n",
    "# Filter for search_result == 0 and reset index\n",
    "df_housing_searched_selected = df_housing_searched[df_housing_searched['search_result'] == 0].reset_index(drop=True)\n",
    "\n",
    "# Add the 'property_type' column\n",
    "df_housing_searched_selected['property_type'] = housing_df['property_type']\n",
    "\n",
    "# Save the filtered dataset\n",
    "df_housing_searched_selected.to_parquet(\"../data/L2/housing_unique_searched.parquet\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "demo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
