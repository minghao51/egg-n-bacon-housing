{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# procesing all utilities with additional latlon and discard extra info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "# import polars as pl\n",
    "import geopandas as gpd\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OneMap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import os\n",
    "import json\n",
    "\n",
    "url = \"https://www.onemap.gov.sg/api/auth/post/getToken\"\n",
    "      \n",
    "payload = {\n",
    "        \"email\": os.environ['ONEMAP_EMAIL'],\n",
    "        \"password\": os.environ['ONEMAP_EMAIL_PASSWORD']\n",
    "      }\n",
    "      \n",
    "response = requests.request(\"POST\", url, json=payload)\n",
    "access_token = json.loads(response.text)['access_token']\n",
    "headers = {\"Authorization\": f\"{access_token}\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# School"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "school = pd.read_csv(\"../data/raw_data/csv/datagov/Generalinformationofschools.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "import pandas as pd\n",
    "import time\n",
    "import random\n",
    "\n",
    "df_list = []\n",
    "max_retries = 3\n",
    "initial_backoff = 1  # seconds\n",
    "max_backoff = 32  # seconds\n",
    "\n",
    "for search_string in school['postal_code']:\n",
    "    # print(search_string)\n",
    "    retries = 0\n",
    "    success = False\n",
    "    backoff = initial_backoff\n",
    "\n",
    "    while not success and retries < max_retries:\n",
    "        try:\n",
    "            url = f\"https://www.onemap.gov.sg/api/common/elastic/search?searchVal={search_string}&returnGeom=Y&getAddrDetails=Y&pageNum=1\"\n",
    "            response = requests.request(\"GET\", url, headers=headers)\n",
    "            response.raise_for_status()  # Raise an exception for HTTP errors\n",
    "\n",
    "            _df = pd.DataFrame(json.loads(response.text)['results']).iloc[0:1]#reset_index().rename({'index':'search_result'}, axis=1)\n",
    "            _df['NameAddress'] = search_string\n",
    "            df_list.append(_df)\n",
    "\n",
    "            success = True\n",
    "\n",
    "        except requests.RequestException as e:\n",
    "            retries += 1\n",
    "            backoff = min(backoff * 2, max_backoff)  # Exponential backoff\n",
    "            delay = backoff + random.uniform(0, 1)  # Add some jitter to the delay\n",
    "            print(f\"Request failed for {search_string}. Retrying in {delay:.2f} seconds. (Retry {retries}/{max_retries})\")\n",
    "            time.sleep(delay)\n",
    "\n",
    "    if not success:\n",
    "        print(f\"Failed to retrieve data for {search_string} after {max_retries} retries.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_school = pd.concat(df_list).rename({'SEARCHVAL':'name','LATITUDE':'lat','LONGITUDE':'lon','POSTAL':'postal','ADDRESS':'address'}, axis=1)\n",
    "df_school = df_school[['name','lat','lon','postal','address']]\n",
    "df_school['type'] = 'school'\n",
    "df_school['address'] = [i.lower() for i in df_school['address']]\n",
    "df_school['name'] = [i.lower() for i in df_school['name']]\n",
    "\n",
    "school_df = df_school"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kindergardens, gym, hawker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "\n",
    "def extract_html_name(html_str: str, name_search) -> str:\n",
    "    \"\"\"\n",
    "    Extracts the name from an HTML string.\n",
    "\n",
    "    Args:\n",
    "        html_str (str): The HTML string to parse.\n",
    "\n",
    "    Returns:\n",
    "        str: The extracted name.\n",
    "    \"\"\"\n",
    "    soup = BeautifulSoup(html_str, 'html.parser')\n",
    "    name_cell = soup.find('th', string=name_search).find_next('td')\n",
    "    name = name_cell.text.strip()\n",
    "    return name\n",
    "\n",
    "def parse_datagov_geojson(path: str, data_type: str, name_search:str='NAME') -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Parses a GeoJSON file and extracts relevant data.\n",
    "\n",
    "    Args:\n",
    "        path (str): The file path to the GeoJSON file.\n",
    "        data_type (str): The type of data (e.g., \"kindergardens\").\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: A DataFrame with the extracted data.\n",
    "    \"\"\"\n",
    "    df = gpd.read_file(path)\n",
    "    df = df.to_crs('4326')\n",
    "    df[\"lat\"] = df['geometry'].y\n",
    "    df[\"lon\"] = df['geometry'].x\n",
    "    df['type'] = data_type\n",
    "    df['name'] = [extract_html_name(i, name_search) for i in df['Description']]\n",
    "    return pd.DataFrame(df[['name', 'type', 'lat', 'lon']])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "kindergarden_df = parse_datagov_geojson(\"../data/raw_data/csv/datagov/Kindergartens.geojson\", \"kindergarden\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "gym_df = parse_datagov_geojson(\"../data/raw_data/csv/datagov/GymsSGGEOJSON.geojson\", \"gym\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "hawker_df = parse_datagov_geojson(\"../data/raw_data/csv/datagov/HawkerCentresGEOJSON.geojson\", \"hawker\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "water_activities_df = parse_datagov_geojson(\"../data/raw_data/csv/datagov/WaterActivitiesSG.geojson\", \"water_activities\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "supermarket_df = parse_datagov_geojson(\"../data/raw_data/csv/datagov/SupermarketsGEOJSON.geojson\", \"supermarket\", \"LIC_NAME\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "preschool_df = parse_datagov_geojson(\"../data/raw_data/csv/datagov/PreSchoolsLocation.geojson\", \"preschool\", \"CENTRE_NAME\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# park"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = gpd.read_file(\"../data/raw_data/csv/datagov/NParksParksandNatureReserves.geojson\")\n",
    "df = df.set_crs(crs='epsg:4326')\n",
    "df = df.to_crs(crs=3857)\n",
    "df['lon'] = df.centroid.x  \n",
    "df['lat'] = df.centroid.y\n",
    "df['type']='park'\n",
    "df['name'] = [extract_html_name(i, 'NAME') for i in df['Description']]\n",
    "park_df = df[['name','type','lon','lat','geometry']]\n",
    "park_df.to_file(\"../data/L1/park.geojson\", driver='GeoJSON')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = gpd.read_file(\"../data/raw_data/csv/datagov/MasterPlan2019SDCPParkConnectorLinelayerGEOJSON.geojson\")\n",
    "park_connector_df = df.drop('Description', axis=1)\n",
    "park_connector_df.to_file(\"../data/L1/park_connector.geojson\", driver='GeoJSON')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "waterbody_df = gpd.read_file(\"../data/raw_data/csv/datagov/MasterPlan2019SDCPWaterbodylayerKML.kml\")\n",
    "waterbody_df = waterbody_df.to_crs('3857')\n",
    "waterbody_df['area_m'] = waterbody_df.geometry.area\n",
    "waterbody_df = waterbody_df[waterbody_df['area_m']>=4000].reset_index()\n",
    "waterbody_df = waterbody_df[['Name','geometry','area_m']]\n",
    "waterbody_df.to_parquet('../data/L1/amenity.parqeut')\n",
    "waterbody_df.to_file(\"../data/L1/waterbody.geojson\", driver='GeoJSON')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Concat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_combined = pd.concat([\n",
    "    school_df[['name','type','lat','lon']],\n",
    "    kindergarden_df,\n",
    "    gym_df,\n",
    "    hawker_df,\n",
    "    kindergarden_df,\n",
    "    water_activities_df,\n",
    "    supermarket_df,\n",
    "    preschool_df\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_combined['lat'] = df_combined['lat'].astype('float')\n",
    "df_combined['lon'] = df_combined['lon'].astype('float')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_combined.to_parquet('../data/L1/amenity.parqeut')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "demo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
