#!/usr/bin/env python3
"""
Pipeline runner script.

This script orchestrates all data pipeline stages:
- L0: Data collection from external APIs
- L1: Data processing and geocoding
- L2: Feature engineering (rental yields and property features)
- L3: Export and final output

Usage:
    uv run python scripts/run_pipeline.py --stage L0
    uv run python scripts/run_pipeline.py --stage L1
    uv run python scripts/run_pipeline.py --stage L2
    uv run python scripts/run_pipeline.py --stage L2_rental
    uv run python scripts/run_pipeline.py --stage L2_features
    uv run python scripts/run_pipeline.py --stage L3
    uv run python scripts/run_pipeline.py --stage all

Examples:
    # Run L0 data collection
    python scripts/run_pipeline.py --stage L0

    # Run L1 processing with parallel geocoding
    python scripts/run_pipeline.py --stage L1 --parallel

    # Run L2 features only
    python scripts/run_pipeline.py --stage L2_features

    # Run all stages
    python scripts/run_pipeline.py --stage all --parallel
"""

import logging
import sys
import argparse
from pathlib import Path

# Add project root to path
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

from scripts.core.config import Config
from scripts.core.pipeline.L0_collect import collect_all_datagovsg
from scripts.core.pipeline.L1_process import run_processing_pipeline, save_failed_addresses
from scripts.core.pipeline.L2_rental import run_rental_pipeline
from scripts.core.pipeline.L2_features import run_features_pipeline
from scripts.core.pipeline.L3_export import run_export_pipeline
from scripts.core.data_helpers import list_datasets

# Setup logging
logging.basicConfig(
    level=logging.INFO, format="%(asctime)s - %(name)s - %(levelname)s - %(message)s"
)
logger = logging.getLogger(__name__)


def run_L0_collection():
    """Run L0: Data collection from external APIs."""
    logger.info("üöÄ Starting L0: Data Collection")

    # Run data.gov.sg collection
    results = collect_all_datagovsg()

    logger.info(f"‚úÖ L0 Complete: Collected {len(results)} datasets")
    return results


def run_L1_processing(use_parallel: bool = True):
    """Run L1: Data processing including geocoding."""
    logger.info("üöÄ Starting L1: Data Processing")

    # Run full L1 pipeline using extracted module
    results = run_processing_pipeline(
        csv_base_path=Config.MANUAL_DIR / "csv", use_parallel_geocoding=use_parallel
    )

    # Save failed addresses for later retry
    if results.get("failed_addresses"):
        # Note: We only save first 10 in results dict
        # For full list, you'd need to modify the pipeline to return all
        pass

    logger.info("‚úÖ L1 Complete")
    return results


def run_L2_rental(force: bool = False):
    """Run L2: Rental yield calculations."""
    logger.info("üöÄ Starting L2: Rental Yield Pipeline")

    results = run_rental_pipeline(force=force)

    logger.info("‚úÖ L2 Rental Complete")
    return results


def run_L2_features():
    """Run L2: Property feature engineering."""
    logger.info("üöÄ Starting L2: Feature Engineering Pipeline")

    results = run_features_pipeline()

    logger.info("‚úÖ L2 Features Complete")
    return results


def run_L2(force: bool = False):
    """Run L2: All L2 pipelines (rental + features)."""
    logger.info("üöÄ Starting L2: Full L2 Pipeline")

    results = {}

    results["rental"] = run_rental_pipeline(force=force)
    results["features"] = run_features_pipeline()

    logger.info("‚úÖ L2 Complete")
    return results


def run_L3(upload_s3: bool = False, export_csv: bool = False):
    """Run L3: Export and final output."""
    logger.info("üöÄ Starting L3: Export Pipeline")

    results = run_export_pipeline(upload_s3=upload_s3, export_csv=export_csv)

    logger.info("‚úÖ L3 Complete")
    return results


def run_pipeline(
    stages: str,
    use_parallel: bool = True,
    force: bool = False,
    upload_s3: bool = False,
    export_csv: bool = False,
):
    """
    Run the data pipeline.

    Args:
        stages: Which stages to run (L0, L1, L2, L2_rental, L2_features, L3, or 'all')
        use_parallel: Whether to use parallel processing for geocoding
        force: Force re-download for rental data
        upload_s3: Upload outputs to S3 (L3 only)
        export_csv: Export outputs to CSV (L3 only)
    """
    logger.info("ü•ìü•ö Egg-n-Bacon Housing Pipeline Starting")
    logger.info(f"Configuration: stages={stages}, parallel={use_parallel}")

    # Validate configuration
    try:
        Config.validate()
    except ValueError as e:
        logger.error(f"‚ùå Configuration error: {e}")
        sys.exit(1)

    # Print configuration
    Config.print_config()

    # Run requested stages
    if stages in ["L0", "all"]:
        logger.info("=" * 80)
        run_L0_collection()

    if stages in ["L1", "all"]:
        logger.info("=" * 80)
        run_L1_processing(use_parallel=use_parallel)

    if stages in ["L2", "all"]:
        logger.info("=" * 80)
        run_L2(force=force)

    if stages == "L2_rental":
        logger.info("=" * 80)
        run_L2_rental(force=force)

    if stages == "L2_features":
        logger.info("=" * 80)
        run_L2_features()

    if stages in ["L3", "all"]:
        logger.info("=" * 80)
        run_L3(upload_s3=upload_s3, export_csv=export_csv)

    # Summary
    logger.info("=" * 80)
    logger.info("üéâ Pipeline complete!")

    # List all datasets
    datasets = list_datasets()
    logger.info(f"üìä Total datasets: {len(datasets)}")
    for name, info in datasets.items():
        logger.info(f"  - {name}: {info['rows']} rows")


def main():
    """Main entry point."""
    parser = argparse.ArgumentParser(description="Run the Egg-n-Bacon Housing data pipeline")
    parser.add_argument(
        "--stage",
        choices=["L0", "L1", "L2", "L2_rental", "L2_features", "L3", "all"],
        default="all",
        help="Which pipeline stage to run (default: all)",
    )
    parser.add_argument(
        "--parallel", action="store_true", help="Use parallel processing for geocoding (faster)"
    )
    parser.add_argument(
        "--no-parallel",
        action="store_true",
        help="Disable parallel processing for geocoding (slower but safer)",
    )
    parser.add_argument(
        "--force", action="store_true", help="Force re-download of rental data even if fresh"
    )
    parser.add_argument("--upload-s3", action="store_true", help="Upload L3 outputs to S3")
    parser.add_argument("--export-csv", action="store_true", help="Export L3 outputs to CSV")

    args = parser.parse_args()

    # Determine whether to use parallel processing
    use_parallel = args.parallel and not args.no_parallel

    # Run pipeline
    try:
        run_pipeline(
            stages=args.stage,
            use_parallel=use_parallel,
            force=args.force,
            upload_s3=args.upload_s3,
            export_csv=args.export_csv,
        )
    except Exception as e:
        logger.exception(f"‚ùå Pipeline failed: {e}")
        sys.exit(1)


if __name__ == "__main__":
    main()
